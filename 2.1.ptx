<?xml version="1.0" encoding="utf-8"?>

<pretext>
  <worksheet xml:id="symmetric-matrices">
  <title>Chapter 2</title>
  <plaintitle>Symmetric Matrices and the SVD</plaintitle>
  <!--Page 40 -->
     <page xml:id="p40" title="2.1 orthogonal Diagonalization">
     <section xml:id="orthogonal-diagonalization" title="2.1 orthogonal Diagonalization">
     <p>
     Many algorithms rely on a type of matrix that is equal to its transpose. If Matrix <m>A</m> satisfies <m>A=A^{T}</m>, then <m>A</m> is <term>symmetric</term>.
     A common example of a symmetric matrix is the product of <m>A^{T}A</m>, where <m>A</m> is in any <m>m \times n</m> matrix. We use <m>A^{T}A</m> when, for example,
     constructing the normal equations in least-squares problems. One way to see that <m>A^{T}A</m> is symmetric for any matric <m>A</m> is to take the transpose of <m>A</m>.
     </p>
     
     <me>
     (A^T A)^T = A^T (A^T)^T = A^T A
     </me>

     <p>
     <m>A^{T} A</m> is equal to its transpose so it must be symmetric. But another way we see that <m>A^{T}A</m> is that for any rectangular matrix <m>A</m> with columns
     <m>a_1, a_2, \ldots, a_n</m>, is to express the matrix product using the row-column rule for matrix multiplication.
     </p>
     
     <md>
    A^T A =
    \begin{pmatrix}
    a_1 \\
    a_2 \\
    \vdots \\
    a_n
    \end{pmatrix}
    \begin{pmatrix}
    a_1 & a_2 & \cdots & a_n
    \end{pmatrix}
    =
    \underbrace{
    \begin{pmatrix}
    a_1 a_1 & a_1 a_2 & \cdots & a_1 a_n \\
    a_2 a_1 & a_2 a_2 & \cdots & a_2 a_n \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    a_n a_1 & a_n a_2 & \cdots & a_n a_n
    \end{pmatrix}
    }_{\text{entries are dot products of columns of } A}
    </md>


    <p>Note that <m>a_i^{T}a_j</m> is the dot product between <m>a_i</m> and <m>a_j</m>. And because dot products</p>
    </section>
    
    </page>

  <!-- Page 41 -->
    <page xml:id="p41">

   
    
     <p>
      commute, in other words
     </p>
     
    <me>a_i^{T}a_j = a_i \cdot a_j = a_j \cdot a_i = a_j^{T}a_i</me>

    <p><m>A^{T}A</m> is symmetric.</p>

    <p>
    One of the reasons that symmetric matrices are found  in many algorithms is that they posses several properties that we can use to make useful or efficient calculations.
    In this section we investigate some of these properties that symmetric matrices have. In later sections of this chapter we will use these properties to develop and
    understand algorithms and their results.
    </p>

    <section xml:id="properties-matrices" title="2.1.1 Properties of Symmetric matrices">

    <p>In this section we give three theorems that characterize symmetric matrices.</p>

      <subsection xml:id="orthogonal-eigenspaces"
                title="1) Symmetric Matrices Have Orthogonal Eigenspaces">
      <p>The eigenspaces of symmetric matrices have a useful property that we can use when, for example, diagonalizing a matrix.</p>
  
      <theorem xml:id="orthogonal-eigenvectors">
        <statement>
        If <m>A</m> is a symmetric matrix, with eigenvectors <m>\vec{v}_1</m> and <m>\vec{v}_2</m> corresponding to two distinct eigenvalues, 
        then <m>\vec{v}_1</m> and <m>\vec{v}_2</m> are orthogonal.
        </statement>
      </theorem>

        <p>
        More generally this theorem implies that eigenspaces associated
        to distinct eigenvalues are orthogonal subspaces.
        </p>

        <proof>
          <p>
            Our approach will be to show that if <m>A</m> is symmetric then
            any two of its eigenvectors <m>\vec{v}_1</m> and
            <m>\vec{v}_2</m> must be orthogonal when their corresponding
            eigenvalues <m>\lambda_1</m> and <m>\lambda_2</m>
          </p>
       </proof>

    </subsection>

  </section>

</page>

<page xml:id="p42">

  <proof>

    <p>
      <m>\lambda_2</m> are not equal to each other.
    </p>
    <md>
  <m>
    \lambda_1 \vec{v}_1 \cdot \vec{v}_2
    = A\vec{v}_1 \cdot \vec{v}_2
    &amp; \mtext{using } A\vec{v}_1 = \lambda_1 \vec{v}_1
  </m>

  <m>
    = (A\vec{v}_1)^T \vec{v}_2
    &amp; \mtext{using the definition of the dot product}
  </m>

  <m>
    = \vec{v}_1^T A^T \vec{v}_2
    &amp; \mtext{property of transpose of product}
  </m>

  <m>
    = \vec{v}_1^T A \vec{v}_2
    &amp; \mtext{given that } A = A^T
  </m>

  <m>
    = \vec{v}_1 \cdot A\vec{v}_2
    &amp; \mtext{again using the definition of the dot product}
  </m>

  <m>
    = \vec{v}_1 \cdot \lambda_2 \vec{v}_2
    &amp; \mtext{using } A\vec{v}_2 = \lambda_2 \vec{v}_2
  </m>
  <m>
    =\lambda_2 \vec{v}_1 \cdot \vec{v}_2
  </m>
</md>

  <p> Rearranging the equation yields</p>

  <me> 
0 = \lambda_2 \vec{v}_1 \cdot \vec{v}_2 - \lambda_1 \vec{v}_1 \cdot \vec{v}_2 = (\lambda_2 - \lambda_1) \vec{v}_1 \cdot \vec{v}_2 
</me>

  <p>
But <m>\lambda_1 \neq \lambda_2</m> so <m>\vec{v}_1 \cdot \vec{v}_2 = 0</m>. In other words, eigenvectors corresponding to distinct eigenvalues must be orthogonal.
</p>

  <p>
This theorem can sometimes be used  to quickly identify the eigenvectors of a matrix. For example, if <m>A</m> is a <m>2 \times 2</m> matrix and we know that <m>\vec{v}_1</m>
is an eigenvector of <m>A</m>, then we can find any non-zero vector orthogonal to <m>\vec{v}_2</m> to identify the eigenvector for the other eigenspace.
</p>

</proof>
<heading level="3">
    2) The Eigenvalues of a Symmetric Matrix are Real
  </heading>
<theorem xml:id="eigenvalues-matrix">
  <statement>
  If <m>A</m> is a real symmetrical matrix then all eigenvalues of <m>A</m> are real.
  </statement>
</theorem>

<p>A proof of this result is in Appendix 1.2</p>

<heading level="3">
    3) The Spectral Theorem
  </heading>
<p>
It turns out that every real symmetric matrix can be diagonalized using an othagonal matrix , which is a result of the spectral theorem.
</p>
</page>

<!-- Aditi write here!! -->

<page xml:id="p43" title="Examples of Orthogonal Diagonalization">

  <section xml:id="examples" title="Examples">

    <example xml:id="example-2x2">
      <title>Example 1: Orthogonal Diagonalization of a 2 × 2 Matrix</title>

      <p>
        Suppose <m>A</m> is the symmetric matrix below.
      </p>

      <me>
        A =
        \begin{pmatrix}
          0 & -2 \\
          -2 & 3
        \end{pmatrix},
        \quad \lambda_1 = 4, \quad \lambda_2 = -1
      </me>

      <p>
        The eigenvalues of <m>A</m> are given. For eigenvalue <m>\lambda_1 = 4</m> we compute
      </p>

      <md>
        A - \lambda_1 I =
        \begin{pmatrix}
          -4 & -2 \\
          -2 & -1
        \end{pmatrix}
      </md>

      <p>
        A vector in the null space of <m>A - \lambda_1 I</m> is the eigenvector
      </p>

      <me>
        \vec{v}_1 =
        \begin{pmatrix}
          1 \\
          -2
        \end{pmatrix}
      </me>

      <p>
        A vector orthogonal to <m>\vec{v}_1</m> is
      </p>

      <me>
        \vec{v}_2 =
        \begin{pmatrix}
          2 \\
          1
        \end{pmatrix}
      </me>

      <p>
        Dividing each eigenvector by its length and collecting them into a matrix <m>P</m>,
        we obtain an orthogonal matrix.
      </p>

      <me>
        A = PDP^{T},
        \quad
        D =
        \begin{pmatrix}
          4 & 0 \\
          0 & -1
        \end{pmatrix},
        \quad
        P = \frac{1}{\sqrt{5}}
        \begin{pmatrix}
          1 & 2 \\
          -2 & 1
        \end{pmatrix}
      </me>

    </example>

  </section>
</page>

<page xml:id="p44" title="Orthogonal Diagonalization of a 3 × 3 Matrix">

  <example xml:id="example-3x3">
    <title>Example 2: Orthogonal Diagonalization of a 3 × 3 Matrix</title>

    <p>
      In this example we will diagonalize a matrix <m>A</m> using an orthogonal matrix <m>P</m>.
    </p>

    <me>
      A =
      \begin{pmatrix}
        0 & 0 & 1 \\
        0 & 1 & 0 \\
        1 & 0 & 0
      \end{pmatrix},
      \quad \lambda = -1, 1
    </me>

    <p>
      For eigenvalue <m>\lambda_1 = -1</m> we compute
    </p>

    <md>
      A - \lambda_1 I =
      \begin{pmatrix}
        1 & 0 & 1 \\
        0 & 2 & 0 \\
        1 & 0 & 1
      \end{pmatrix}
    </md>

    <p>
      A vector in the null space of <m>A - \lambda_1 I</m> is
    </p>

    <me>
      \vec{v}_1 =
      \begin{pmatrix}
        1 \\
        0 \\
        -1
      \end{pmatrix}
    </me>

    <p>
      For eigenvalue <m>\lambda_2 = 1</m> we compute
    </p>

    <md>
      A - \lambda_2 I =
      \begin{pmatrix}
        -1 & 0 & 1 \\
        0 & 0 & 0 \\
        1 & 0 & -1
      \end{pmatrix}
    </md>

    <p>
      Two vectors in the null space are
    </p>

    <me>
      \vec{v}_2 =
      \begin{pmatrix}
        0 \\
        1 \\
        0
      \end{pmatrix},
      \quad
      \vec{v}_3 =
      \begin{pmatrix}
        1 \\
        0 \\
        1
      \end{pmatrix}
    </me>

    <p>
      These vectors are orthogonal. Dividing each eigenvector by its length and
      collecting them into a matrix <m>P</m> gives
    </p>

    <me>
      A = PDP^{T},
      \quad
      D =
      \begin{pmatrix}
        -1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
      \end{pmatrix},
      \quad
      P = \frac{1}{\sqrt{2}}
      \begin{pmatrix}
        1 & 0 & 1 \\
        0 & \sqrt{2} & 0 \\
        -1 & 0 & 1
      \end{pmatrix}
    </me>

  </example>
</page>

<page xml:id="p45" title="Summary and Exercises">

  <section xml:id="summary" title="2.1.3 Summary">

    <p>
      In this section we explored how to construct an orthogonal diagonalization
      of a symmetric matrix <m>A = PDP^{T}</m>.
    </p>

    <ul>
      <li>All eigenvalues of <m>A</m> are real</li>
      <li>Eigenspaces of <m>A</m> are mutually orthogonal</li>
      <li><m>A</m> can be diagonalized as <m>A = PDP^{T}</m></li>
    </ul>

  </section>

  <section xml:id="exercises" title="2.1.4 Exercises">

    <ol>
      <li>
        Suppose <m>A</m> and <m>C</m> are <m>n \times n</m> matrices,
        <m>\vec{x} \in \mathbb{R}^n</m>, and <m>C</m> is symmetric.
        Which of the following products are symmetric?
        <ol>
          <li><m>AA^{T}</m></li>
          <li><m>\vec{x}\vec{x}^{T}</m></li>
          <li><m>C^2</m></li>
        </ol>
      </li>

      <li>
        If <m>A = PDP^{T}</m> where <m>D</m> is diagonal and <m>P^{T} = P^{-1}</m>,
        is <m>A</m> symmetric?
      </li>
    </ol>

  </section>

</page>
</worksheet>
</pretext>

